{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:00.094896Z",
     "start_time": "2025-01-10T11:15:00.063491Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1) Load the HCC dataset\n",
    "# Adjust 'delimiter' and 'decimal' parameters if necessary to match your file format\n",
    "df = pd.read_csv(\"hcc-data-complete-balanced.csv\", delimiter=\",\", decimal=\",\")\n",
    "\n",
    "# Quick check of the data\n",
    "print(df.head())\n",
    "print(df.columns)\n",
    "print(df.info())\n",
    "\n",
    "# Drop the target column to get features\n",
    "X = df.drop(columns=[\"Class\"])\n",
    "y = df[\"Class\"]\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Gender  Symptoms  Alcohol  HBsAg  HBeAg  HBcAb  HCVAb  Cirrhosis  Endemic  \\\n",
      "0       1         0        1      0      0      0      0          1        0   \n",
      "1       0         0        0      0      0      0      1          1        0   \n",
      "2       1         0        1      1      0      1      0          1        0   \n",
      "3       1         1        1      0      0      0      0          1        0   \n",
      "4       1         1        1      1      0      1      0          1        0   \n",
      "\n",
      "   Smoking  ...  ALP   TP  Creatinine  Nodule  Major_Dim  Dir_Bil   Iron  \\\n",
      "0        1  ...  150  7.1        0.70       1        3.5     0.50   52.5   \n",
      "1        1  ...  120  7.0        0.58       1        1.8     0.85   32.0   \n",
      "2        1  ...  109  7.0        2.10       5       13.0     0.10   28.0   \n",
      "3        1  ...  174  8.1        1.11       2       15.7     0.20  131.0   \n",
      "4        1  ...  109  6.9        1.80       1        9.0     0.10   59.0   \n",
      "\n",
      "    Sat  Ferritin  Class  \n",
      "0  37.0     856.0      1  \n",
      "1  10.0      18.0      1  \n",
      "2   6.0      16.0      1  \n",
      "3  78.0    1316.0      0  \n",
      "4  15.0      22.0      1  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "Index(['Gender', 'Symptoms', 'Alcohol', 'HBsAg', 'HBeAg', 'HBcAb', 'HCVAb',\n",
      "       'Cirrhosis', 'Endemic', 'Smoking', 'Diabetes', 'Obesity', 'Hemochro',\n",
      "       'AHT', 'CRI', 'HIV', 'NASH', 'Varices', 'Spleno', 'PHT', 'PVT',\n",
      "       'Metastasis', 'Hallmark', 'Age', 'Grams_day', 'Packs_year', 'PS',\n",
      "       'Encephalopathy', 'Ascites', 'INR', 'AFP', 'Hemoglobin', 'MCV',\n",
      "       'Leucocytes', 'Platelets', 'Albumin', 'Total_Bil', 'ALT', 'AST', 'GGT',\n",
      "       'ALP', 'TP', 'Creatinine', 'Nodule', 'Major_Dim', 'Dir_Bil', 'Iron',\n",
      "       'Sat', 'Ferritin', 'Class'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 204 entries, 0 to 203\n",
      "Data columns (total 50 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Gender          204 non-null    int64  \n",
      " 1   Symptoms        204 non-null    int64  \n",
      " 2   Alcohol         204 non-null    int64  \n",
      " 3   HBsAg           204 non-null    int64  \n",
      " 4   HBeAg           204 non-null    int64  \n",
      " 5   HBcAb           204 non-null    int64  \n",
      " 6   HCVAb           204 non-null    int64  \n",
      " 7   Cirrhosis       204 non-null    int64  \n",
      " 8   Endemic         204 non-null    int64  \n",
      " 9   Smoking         204 non-null    int64  \n",
      " 10  Diabetes        204 non-null    int64  \n",
      " 11  Obesity         204 non-null    int64  \n",
      " 12  Hemochro        204 non-null    int64  \n",
      " 13  AHT             204 non-null    int64  \n",
      " 14  CRI             204 non-null    int64  \n",
      " 15  HIV             204 non-null    int64  \n",
      " 16  NASH            204 non-null    int64  \n",
      " 17  Varices         204 non-null    int64  \n",
      " 18  Spleno          204 non-null    int64  \n",
      " 19  PHT             204 non-null    int64  \n",
      " 20  PVT             204 non-null    int64  \n",
      " 21  Metastasis      204 non-null    int64  \n",
      " 22  Hallmark        204 non-null    int64  \n",
      " 23  Age             204 non-null    int64  \n",
      " 24  Grams_day       204 non-null    int64  \n",
      " 25  Packs_year      204 non-null    float64\n",
      " 26  PS              204 non-null    int64  \n",
      " 27  Encephalopathy  204 non-null    int64  \n",
      " 28  Ascites         204 non-null    int64  \n",
      " 29  INR             204 non-null    float64\n",
      " 30  AFP             204 non-null    float64\n",
      " 31  Hemoglobin      204 non-null    float64\n",
      " 32  MCV             204 non-null    float64\n",
      " 33  Leucocytes      204 non-null    float64\n",
      " 34  Platelets       204 non-null    float64\n",
      " 35  Albumin         204 non-null    float64\n",
      " 36  Total_Bil       204 non-null    float64\n",
      " 37  ALT             204 non-null    int64  \n",
      " 38  AST             204 non-null    int64  \n",
      " 39  GGT             204 non-null    int64  \n",
      " 40  ALP             204 non-null    int64  \n",
      " 41  TP              204 non-null    float64\n",
      " 42  Creatinine      204 non-null    float64\n",
      " 43  Nodule          204 non-null    int64  \n",
      " 44  Major_Dim       204 non-null    float64\n",
      " 45  Dir_Bil         204 non-null    float64\n",
      " 46  Iron            204 non-null    float64\n",
      " 47  Sat             204 non-null    float64\n",
      " 48  Ferritin        204 non-null    float64\n",
      " 49  Class           204 non-null    int64  \n",
      "dtypes: float64(16), int64(34)\n",
      "memory usage: 79.8 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:00.653451Z",
     "start_time": "2025-01-10T11:15:00.635322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,      # 20% for testing\n",
    "    stratify=y,         # preserve class distribution\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Recombine for convenience\n",
    "train_df = X_train.copy()\n",
    "train_df[\"Class\"] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[\"Class\"] = y_test\n",
    "\n",
    "num_hospitals = 20\n",
    "\n",
    "# Shuffle the training data\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "chunk_size = len(train_df) // num_hospitals\n",
    "hospital_data = []\n",
    "\n",
    "for i in range(num_hospitals):\n",
    "    start_idx = i * chunk_size\n",
    "\n",
    "    # last hospital gets the remainder if not evenly divisible\n",
    "    if i < num_hospitals - 1:\n",
    "        end_idx = (i + 1) * chunk_size\n",
    "    else:\n",
    "        end_idx = len(train_df)\n",
    "\n",
    "    # slice of data for this hospital\n",
    "    hospital_df = train_df.iloc[start_idx:end_idx].copy()\n",
    "    hospital_data.append(hospital_df)\n",
    "\n",
    "# Now hospital_data is a list of 20 DataFrames\n",
    "# Each DataFrame has the features plus the Class column\n",
    "print(f\"Total training samples: {len(train_df)}\")\n",
    "print([len(d) for d in hospital_data])\n"
   ],
   "id": "de92224665f9a811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 163\n",
      "[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 11]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Part 2: Federated Learning",
   "id": "663aa966fd594116"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:01.360795Z",
     "start_time": "2025-01-10T11:15:01.353695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def my_forest_train(X, y, n_estimators=100, random_state=42):\n",
    "    \"\"\"\n",
    "    Train a Random Forest classifier with n_estimators trees.\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def my_forest_predict(model, X):\n",
    "    \"\"\"\n",
    "    Predict the class labels for the given features X.\n",
    "    \"\"\"\n",
    "    return model.predict(X)\n",
    "\n",
    "def evaluate_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Simple accuracy metric.\n",
    "    \"\"\"\n",
    "    return accuracy_score(y_true, y_pred)\n"
   ],
   "id": "9af6551a57fbf061",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:03.317811Z",
     "start_time": "2025-01-10T11:15:01.943895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "local_forest_models = []\n",
    "\n",
    "for i, hospital_df in enumerate(hospital_data):\n",
    "    X_local = hospital_df.drop(columns=[\"Class\"])\n",
    "    y_local = hospital_df[\"Class\"]\n",
    "\n",
    "    # Train local model\n",
    "    rf_model = my_forest_train(X_local, y_local, n_estimators=50, random_state=42 + i)\n",
    "    local_forest_models.append(rf_model)\n"
   ],
   "id": "a55c050d9e981680",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "10de76d9a11f12fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:22.353363Z",
     "start_time": "2025-01-10T11:15:22.202807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_test_only = test_df.drop(columns=[\"Class\"])\n",
    "y_test_only = test_df[\"Class\"]\n",
    "\n",
    "local_accuracies = []\n",
    "for i, model in enumerate(local_forest_models):\n",
    "    y_pred_local = my_forest_predict(model, X_test_only)\n",
    "    acc_local = evaluate_accuracy(y_test_only, y_pred_local)\n",
    "    local_accuracies.append(acc_local)\n",
    "    print(f\"Hospital {i+1} - Local Model Accuracy on Test Set: {acc_local:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage local model accuracy: {sum(local_accuracies)/len(local_accuracies):.4f}\")\n"
   ],
   "id": "645ef780b1140ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hospital 1 - Local Model Accuracy on Test Set: 0.5854\n",
      "Hospital 2 - Local Model Accuracy on Test Set: 0.5122\n",
      "Hospital 3 - Local Model Accuracy on Test Set: 0.6341\n",
      "Hospital 4 - Local Model Accuracy on Test Set: 0.5122\n",
      "Hospital 5 - Local Model Accuracy on Test Set: 0.5122\n",
      "Hospital 6 - Local Model Accuracy on Test Set: 0.4878\n",
      "Hospital 7 - Local Model Accuracy on Test Set: 0.6098\n",
      "Hospital 8 - Local Model Accuracy on Test Set: 0.5122\n",
      "Hospital 9 - Local Model Accuracy on Test Set: 0.7561\n",
      "Hospital 10 - Local Model Accuracy on Test Set: 0.5122\n",
      "Hospital 11 - Local Model Accuracy on Test Set: 0.5854\n",
      "Hospital 12 - Local Model Accuracy on Test Set: 0.6098\n",
      "Hospital 13 - Local Model Accuracy on Test Set: 0.7073\n",
      "Hospital 14 - Local Model Accuracy on Test Set: 0.6829\n",
      "Hospital 15 - Local Model Accuracy on Test Set: 0.5854\n",
      "Hospital 16 - Local Model Accuracy on Test Set: 0.7317\n",
      "Hospital 17 - Local Model Accuracy on Test Set: 0.6585\n",
      "Hospital 18 - Local Model Accuracy on Test Set: 0.5610\n",
      "Hospital 19 - Local Model Accuracy on Test Set: 0.5854\n",
      "Hospital 20 - Local Model Accuracy on Test Set: 0.6341\n",
      "\n",
      "Average local model accuracy: 0.5988\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-10T11:15:42.424896Z",
     "start_time": "2025-01-10T11:15:42.283030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def federated_predict(models, X):\n",
    "    \"\"\"\n",
    "    Each model predicts; we take a majority vote among the predictions.\n",
    "    \"\"\"\n",
    "    # shape: (num_models, num_samples)\n",
    "    all_preds = np.array([my_forest_predict(m, X) for m in models])\n",
    "    # majority vote along axis=0\n",
    "    # for each sample, all_preds[:, sample_idx] has predictions from all models\n",
    "    final_preds = []\n",
    "\n",
    "    for sample_idx in range(all_preds.shape[1]):\n",
    "        # get the column of predictions for this sample\n",
    "        votes = all_preds[:, sample_idx]\n",
    "        # majority vote\n",
    "        values, counts = np.unique(votes, return_counts=True)\n",
    "        majority_label = values[np.argmax(counts)]\n",
    "        final_preds.append(majority_label)\n",
    "\n",
    "    return np.array(final_preds)\n",
    "\n",
    "# Evaluate federated model\n",
    "y_pred_federated = federated_predict(local_forest_models, X_test_only)\n",
    "federated_accuracy = evaluate_accuracy(y_test_only, y_pred_federated)\n",
    "print(f\"\\nFederated Model Accuracy on Test Set: {federated_accuracy:.4f}\")\n"
   ],
   "id": "77e480c9ad23aa5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Federated Model Accuracy on Test Set: 0.8537\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Hyperparameters\n",
    "num_clients = 5     # e.g. 5 \"hospitals\"/clients\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "num_epochs_local = 1  # local epochs each round\n",
    "num_rounds = 5        # how many federated rounds\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# Shuffle the training dataset indices\n",
    "indices = np.arange(len(train_dataset))\n",
    "np.random.shuffle(indices)\n",
    "split_size = len(indices) // num_clients\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    start = i * split_size\n",
    "    end = (i+1) * split_size if i < num_clients - 1 else len(indices)\n",
    "    subset_idx = indices[start:end]\n",
    "    client_subset = Subset(train_dataset, subset_idx)\n",
    "    client_loader = DataLoader(client_subset, batch_size=batch_size, shuffle=True)\n",
    "    client_loaders.append(client_loader)\n",
    "\n",
    "# Global test loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "id": "dd40570cebbc9a0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # (N, 32, 14, 14)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (N, 64, 7, 7)\n",
    "        x = x.view(x.size(0), -1)             # flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "def scale_params(model, fraction):\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data * fraction\n",
    "def sum_weights(model_list):\n",
    "    \"\"\"\n",
    "    Returns a new model whose weights are the sum of corresponding\n",
    "    parameters from each model in model_list.\n",
    "    \"\"\"\n",
    "    # Deep copy the first model\n",
    "    sum_model = copy.deepcopy(model_list[0])\n",
    "\n",
    "    # Zero out the parameters in sum_model\n",
    "    for param in sum_model.parameters():\n",
    "        param.data = torch.zeros_like(param.data)\n",
    "\n",
    "    # Sum up the parameters from every model in the list\n",
    "    for model in model_list:\n",
    "        for sum_param, model_param in zip(sum_model.parameters(), model.parameters()):\n",
    "            sum_param.data += model_param.data\n",
    "\n",
    "    return\n",
    "def local_train(model, dataloader, epochs, lr=0.01):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for _ in range(epochs):\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Return to CPU for easier manipulation\n",
    "    model.cpu()\n",
    "    return model\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    model.cpu()\n",
    "    return 100.0 * correct / total\n",
    "\n"
   ],
   "id": "181398669ca93a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# fraction alpha\n",
    "fraction = 1.0 / num_clients  # for simple averaging\n",
    "\n",
    "global_model = SimpleCNN(num_classes=10)\n",
    "\n",
    "for round_idx in range(num_rounds):\n",
    "    local_models = []\n",
    "\n",
    "    # Each client trains locally\n",
    "    for client_idx in range(num_clients):\n",
    "        # clone global model for local training\n",
    "        local_model = copy.deepcopy(global_model)\n",
    "\n",
    "        # train locally\n",
    "        local_model = local_train(local_model, client_loaders[client_idx], num_epochs_local, lr)\n",
    "        local_models.append(local_model)\n",
    "\n",
    "    # Sum weights of local models\n",
    "    summed_model = sum_weights(local_models)\n",
    "\n",
    "    # Scale the summed model by the fraction alpha\n",
    "    scale_params(summed_model, fraction)\n",
    "\n",
    "    # This is the new global model\n",
    "    global_model = summed_model\n",
    "\n",
    "    # Evaluate on the global test set\n",
    "    acc = evaluate(global_model, test_loader)\n",
    "    print(f\"Round {round_idx+1}/{num_rounds}, Global Model Accuracy: {acc:.2f}%\")"
   ],
   "id": "c60bfb56df07a70f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
