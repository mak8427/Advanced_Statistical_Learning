{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-05T12:32:34.250353Z"
    }
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to generate a bootstrapped dataset\n",
    "def create_bootstrap_data(inputs, targets):\n",
    "    num_samples = inputs.shape[0]\n",
    "    indices = np.random.choice(range(num_samples), size=num_samples, replace=True)\n",
    "    return inputs[indices], targets[indices]\n",
    "\n",
    "\n",
    "# Function to train multiple decision trees on bootstrapped datasets\n",
    "def train_decision_forest(feature_set, target_set, num_models):\n",
    "    model_list = []\n",
    "    for _ in range(num_models):\n",
    "        sampled_features, sampled_targets = create_bootstrap_data(feature_set, target_set)\n",
    "        tree_model = DecisionTreeClassifier()\n",
    "        tree_model.fit(sampled_features, sampled_targets)\n",
    "        model_list.append(tree_model)\n",
    "    return model_list\n",
    "\n",
    "\n",
    "# Function to aggregate predictions from a decision forest\n",
    "def forest_vote_predict(models, data_to_predict):\n",
    "    all_model_outputs = np.array([model.predict(data_to_predict) for model in models])\n",
    "    majority_vote, _ = mode(all_model_outputs, axis=0)\n",
    "    return majority_vote.flatten()\n",
    "\n",
    "\n",
    "# Function to read data from files and prepare inputs and targets\n",
    "def load_and_prepare_data(file_positive, file_negative):\n",
    "    def parse_file(file_path, class_label):\n",
    "        labels = []\n",
    "        features = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                features.append(list(map(float, parts[1:])))\n",
    "                labels.append(class_label)\n",
    "        return np.array(labels), np.array(features)\n",
    "\n",
    "    positive_labels, positive_features = parse_file(file_positive, 1)\n",
    "    negative_labels, negative_features = parse_file(file_negative, 0)\n",
    "\n",
    "    all_labels = np.concatenate([positive_labels, negative_labels], axis=0)\n",
    "    all_features = np.vstack([positive_features, negative_features])\n",
    "\n",
    "    shuffle_order = np.random.permutation(all_labels.shape[0])\n",
    "    return all_labels[shuffle_order], all_features[shuffle_order]\n",
    "\n",
    "\n",
    "# File paths and data processing\n",
    "classification_labels, classification_features = load_and_prepare_data('sun_exposed.txt', 'not_sun_exposed.txt')\n",
    "\n",
    "\n",
    "# Train the decision forest\n",
    "trained_forest = train_decision_forest(classification_features, classification_labels, num_models=5)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predicted_outputs = forest_vote_predict(trained_forest, classification_features)\n",
    "print(\"Sample Predictions:\", predicted_outputs[:10])\n",
    "print(\"Training Accuracy:\", np.mean(predicted_outputs == classification_labels))\n",
    "\n",
    "# Analyze forest performance with varying numbers of models\n",
    "tree_numbers = range(1, 21)\n",
    "performance_metrics = []\n",
    "\n",
    "for trees in tqdm(tree_numbers):\n",
    "    forest_model = train_decision_forest(classification_features, classification_labels, num_models=trees)\n",
    "    training_predictions = forest_vote_predict(forest_model, classification_features)\n",
    "    accuracy = np.mean(training_predictions == classification_labels)\n",
    "    performance_metrics.append(accuracy)\n",
    "\n",
    "# Plot the performance analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tree_numbers, performance_metrics, marker='o', linestyle='-', color='purple', label='Training Accuracy')\n",
    "plt.title(\"Forest Accuracy vs Number of Decision Trees\")\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(tree_numbers)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1      2     3      4      5      6      7     8     9   ...  \\\n",
      "0  0.08   0.18   0.00  0.03   0.00   0.00   0.09   0.00  0.00  0.00  ...   \n",
      "1  0.15   0.16   0.00  0.10   0.07   0.11   0.09   0.13  0.09  0.05  ...   \n",
      "2  9.76  11.13  15.09  6.94  13.32  11.25  10.37  10.92  7.70  9.26  ...   \n",
      "3  0.18   0.14   0.02  0.08   0.09   0.11   0.12   0.08  0.02  0.11  ...   \n",
      "4  0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00  0.00  0.00  ...   \n",
      "\n",
      "     90    91     92     93    94     95     96    97     98     99  \n",
      "0  0.05  0.00   0.00   0.08  0.00   0.04   0.00  0.03   0.07   0.00  \n",
      "1  0.07  0.26   0.06   0.19  0.17   0.07   0.17  0.08   0.24   0.08  \n",
      "2  9.51  7.05  11.77  12.75  8.11  13.24  11.30  9.30  13.04  10.99  \n",
      "3  0.10  0.05   0.00   0.09  0.06   0.06   0.16  0.05   0.10   0.17  \n",
      "4  0.00  0.00   0.00   0.00  0.00   0.00   0.00  0.00   0.00   0.00  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "   0\n",
      "0  1\n",
      "1  0\n",
      "2  1\n",
      "3  1\n",
      "4  0\n",
      "Sample Predictions: [1 0 1 1 1 1 0 0 1 1]\n",
      "Training Accuracy: 0.8707326296971155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:25<15:12, 53.70s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Random forest increases the risk of overfitting by avoiding bootstrapping in sampling and keeping the features for splitting deterministic. Without bootstrapping, the trees are constructed on identical datasets, leading to higher correlation among the trees. The lack of randomness in the feature selection makes the construction process deterministic, resulting in: \n",
    "\n",
    "- Higher similarity among the trees, which reduces the diversity of the ensemble.\n",
    "- An increased risk of overfitting to the training data, as the trees become overly specialized to the shared patterns in the data. \n",
    "- A loss of the robustness and generalization advantages typically offered by Random Forest. * -1\n",
    "- "
   ],
   "id": "c2999096f9ece4f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "def my_forest_ada_train(features, labels, n_trees):\n",
    "    num_samples = features.shape[0]\n",
    "    sample_weights = np.ones(num_samples) / num_samples\n",
    "    trained_trees = []\n",
    "    for _ in range(n_trees):\n",
    "        # Train a weak learner\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(features, labels, sample_weight=sample_weights)\n",
    "        predictions = stump.predict(features)\n",
    "        error = np.sum(sample_weights * (predictions != labels)) / np.sum(sample_weights)\n",
    "\n",
    "        if error > 0.5:\n",
    "            break\n",
    "        if error == 0:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "        sample_weights *= np.exp(-alpha * labels * predictions)\n",
    "        sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "        trained_trees.append((stump, alpha))\n",
    "    return trained_trees\n",
    "def my_forest_ada_predict(trained_trees, features):\n",
    "    weighted_votes = np.zeros(features.shape[0])\n",
    "    for tree, weight in trained_trees:\n",
    "        predictions = tree.predict(features)\n",
    "        weighted_votes += weight * predictions\n",
    "    final_predictions = np.sign(weighted_votes)\n",
    "    return final_predictions\n",
    "def my_forest_ada_train_equal_weights(features, labels, n_trees):\n",
    "    trained_trees = []\n",
    "    for _ in range(n_trees):\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(features, labels)\n",
    "        trained_trees.append((stump, 1))  # Equal weight (1) for all trees\n",
    "    return trained_trees\n",
    "def my_forest_ada_predict_equal_weights(trained_trees, features):\n",
    "    votes = np.zeros(features.shape[0])\n",
    "    for tree, _ in trained_trees:\n",
    "        predictions = tree.predict(features)\n",
    "        votes += predictions\n",
    "    final_predictions = np.sign(votes)\n",
    "    return final_predictions\n",
    "def load_data(file1, file2):\n",
    "    def read_file(file_path, label):\n",
    "        features, targets = [], []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                values = list(map(float, line.strip().split('\\t')[1:]))\n",
    "                features.append(values)\n",
    "                targets.append(label)\n",
    "        return np.array(features), np.array(targets)\n",
    "\n",
    "    features1, targets1 = read_file(file1, 1)\n",
    "    features2, targets2 = read_file(file2, -1)\n",
    "\n",
    "    features = np.vstack([features1, features2])\n",
    "    targets = np.concatenate([targets1, targets2])\n",
    "\n",
    "    shuffle_indices = np.random.permutation(len(targets))\n",
    "    return features[shuffle_indices], targets[shuffle_indices]\n",
    "X, Y = load_data('sun_exposed.txt', 'not_sun_exposed.txt')\n",
    "n_trees_range = range(1, 21)\n",
    "standard_accuracies = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_range):\n",
    "    ada_model = my_forest_ada_train(X, Y, n_trees)\n",
    "    predictions = my_forest_ada_predict(ada_model, X)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    standard_accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "equal_weights_accuracies = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_range):\n",
    "    equal_model = my_forest_ada_train_equal_weights(X, Y, n_trees)\n",
    "    predictions = my_forest_ada_predict_equal_weights(equal_model, X)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    equal_weights_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_trees_range, standard_accuracies, marker='o', label='Standard AdaBoost')\n",
    "plt.plot(n_trees_range, equal_weights_accuracies, marker='s', label='Equal Weights AdaBoost')\n",
    "plt.title('Accuracy vs Number of Trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "aff707764a22f11f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observations\n",
    "Standard AdaBoost:\n",
    "\n",
    "Accuracy improves as the number of trees increases, but may plateau or decrease slightly due to overfitting.\n",
    "Dynamic weighting allows the algorithm to focus on harder-to-classify samples, improving robustness.\n",
    "Equal Weights AdaBoost:\n",
    "\n",
    "Accuracy improves initially but plateaus much earlier than Standard AdaBoost.\n",
    "Without adaptive weights, the model cannot emphasize harder-to-classify samples, leading to reduced performance.\n",
    "Impact of Adaptive Weighting:\n",
    "\n",
    "Adaptive weighting enhances AdaBoost's ability to address difficult samples, improving overall accuracy.\n",
    "The equal-weight version essentially mimics a simple ensemble method without the boosting aspect.\n",
    "These differences demonstrate the importance of adaptive sample weighting in AdaBoost's performance."
   ],
   "id": "d1c01a0e62087f7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8eb06949d5b3ff5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
