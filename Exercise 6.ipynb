{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-05T12:32:34.250353Z"
    }
   },
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Function to generate a bootstrapped dataset\n",
    "def create_bootstrap_data(inputs, targets):\n",
    "    num_samples = inputs.shape[0]\n",
    "    indices = np.random.choice(range(num_samples), size=num_samples, replace=True)\n",
    "    return inputs[indices], targets[indices]\n",
    "\n",
    "\n",
    "# Function to train multiple decision trees on bootstrapped datasets\n",
    "def train_decision_forest(feature_set, target_set, num_models):\n",
    "    model_list = []\n",
    "    for _ in range(num_models):\n",
    "        sampled_features, sampled_targets = create_bootstrap_data(feature_set, target_set)\n",
    "        tree_model = DecisionTreeClassifier()\n",
    "        tree_model.fit(sampled_features, sampled_targets)\n",
    "        model_list.append(tree_model)\n",
    "    return model_list\n",
    "\n",
    "\n",
    "# Function to aggregate predictions from a decision forest\n",
    "def forest_vote_predict(models, data_to_predict):\n",
    "    all_model_outputs = np.array([model.predict(data_to_predict) for model in models])\n",
    "    majority_vote, _ = mode(all_model_outputs, axis=0)\n",
    "    return majority_vote.flatten()\n",
    "\n",
    "\n",
    "# Function to read data from files and prepare inputs and targets\n",
    "def load_and_prepare_data(file_positive, file_negative):\n",
    "    def parse_file(file_path, class_label):\n",
    "        labels = []\n",
    "        features = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                features.append(list(map(float, parts[1:])))\n",
    "                labels.append(class_label)\n",
    "        return np.array(labels), np.array(features)\n",
    "\n",
    "    positive_labels, positive_features = parse_file(file_positive, 1)\n",
    "    negative_labels, negative_features = parse_file(file_negative, 0)\n",
    "\n",
    "    all_labels = np.concatenate([positive_labels, negative_labels], axis=0)\n",
    "    all_features = np.vstack([positive_features, negative_features])\n",
    "\n",
    "    shuffle_order = np.random.permutation(all_labels.shape[0])\n",
    "    return all_labels[shuffle_order], all_features[shuffle_order]\n",
    "\n",
    "\n",
    "# File paths and data processing\n",
    "classification_labels, classification_features = load_and_prepare_data('sun_exposed.txt', 'not_sun_exposed.txt')\n",
    "\n",
    "\n",
    "# Train the decision forest\n",
    "trained_forest = train_decision_forest(classification_features, classification_labels, num_models=5)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "predicted_outputs = forest_vote_predict(trained_forest, classification_features)\n",
    "print(\"Sample Predictions:\", predicted_outputs[:10])\n",
    "print(\"Training Accuracy:\", np.mean(predicted_outputs == classification_labels))\n",
    "\n",
    "# Analyze forest performance with varying numbers of models\n",
    "tree_numbers = range(1, 21)\n",
    "performance_metrics = []\n",
    "\n",
    "for trees in tqdm(tree_numbers):\n",
    "    forest_model = train_decision_forest(classification_features, classification_labels, num_models=trees)\n",
    "    training_predictions = forest_vote_predict(forest_model, classification_features)\n",
    "    accuracy = np.mean(training_predictions == classification_labels)\n",
    "    performance_metrics.append(accuracy)\n",
    "\n",
    "# Plot the performance analysis\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(tree_numbers, performance_metrics, marker='o', linestyle='-', color='purple', label='Training Accuracy')\n",
    "plt.title(\"Forest Accuracy vs Number of Decision Trees\")\n",
    "plt.xlabel(\"Number of Trees\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(tree_numbers)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1      2     3      4      5      6      7     8     9   ...  \\\n",
      "0  0.08   0.18   0.00  0.03   0.00   0.00   0.09   0.00  0.00  0.00  ...   \n",
      "1  0.15   0.16   0.00  0.10   0.07   0.11   0.09   0.13  0.09  0.05  ...   \n",
      "2  9.76  11.13  15.09  6.94  13.32  11.25  10.37  10.92  7.70  9.26  ...   \n",
      "3  0.18   0.14   0.02  0.08   0.09   0.11   0.12   0.08  0.02  0.11  ...   \n",
      "4  0.00   0.00   0.00  0.00   0.00   0.00   0.00   0.00  0.00  0.00  ...   \n",
      "\n",
      "     90    91     92     93    94     95     96    97     98     99  \n",
      "0  0.05  0.00   0.00   0.08  0.00   0.04   0.00  0.03   0.07   0.00  \n",
      "1  0.07  0.26   0.06   0.19  0.17   0.07   0.17  0.08   0.24   0.08  \n",
      "2  9.51  7.05  11.77  12.75  8.11  13.24  11.30  9.30  13.04  10.99  \n",
      "3  0.10  0.05   0.00   0.09  0.06   0.06   0.16  0.05   0.10   0.17  \n",
      "4  0.00  0.00   0.00   0.00  0.00   0.00   0.00  0.00   0.00   0.00  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "   0\n",
      "0  1\n",
      "1  0\n",
      "2  1\n",
      "3  1\n",
      "4  0\n",
      "Sample Predictions: [1 0 1 1 1 1 0 0 1 1]\n",
      "Training Accuracy: 0.8707326296971155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [02:25<15:12, 53.70s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Random forest increases the risk of overfitting by avoiding bootstrapping in sampling and keeping the features for splitting deterministic. Without bootstrapping, the trees are constructed on identical datasets, leading to higher correlation among the trees. The lack of randomness in the feature selection makes the construction process deterministic, resulting in: \n",
    "\n",
    "- Higher similarity among the trees, which reduces the diversity of the ensemble.\n",
    "- An increased risk of overfitting to the training data, as the trees become overly specialized to the shared patterns in the data. \n",
    "- A loss of the robustness and generalization advantages typically offered by Random Forest. * -1\n",
    "- "
   ],
   "id": "c2999096f9ece4f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:50:29.661366Z",
     "start_time": "2024-12-06T12:49:52.317492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "def my_forest_ada_train(features, labels, n_trees):\n",
    "    num_samples = features.shape[0]\n",
    "    sample_weights = np.ones(num_samples) / num_samples\n",
    "    trained_trees = []\n",
    "    for _ in range(n_trees):\n",
    "        # Train a weak learner\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(features, labels, sample_weight=sample_weights)\n",
    "        predictions = stump.predict(features)\n",
    "        error = np.sum(sample_weights * (predictions != labels)) / np.sum(sample_weights)\n",
    "\n",
    "        if error > 0.5:\n",
    "            break\n",
    "        if error == 0:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "        sample_weights *= np.exp(-alpha * labels * predictions)\n",
    "        sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "        trained_trees.append((stump, alpha))\n",
    "    return trained_trees\n",
    "def my_forest_ada_predict(trained_trees, features):\n",
    "    weighted_votes = np.zeros(features.shape[0])\n",
    "    for tree, weight in trained_trees:\n",
    "        predictions = tree.predict(features)\n",
    "        weighted_votes += weight * predictions\n",
    "    final_predictions = np.sign(weighted_votes)\n",
    "    return final_predictions\n",
    "def my_forest_ada_train_equal_weights(features, labels, n_trees):\n",
    "    trained_trees = []\n",
    "    for _ in range(n_trees):\n",
    "        stump = DecisionTreeClassifier(max_depth=1)\n",
    "        stump.fit(features, labels)\n",
    "        trained_trees.append((stump, 1))  # Equal weight (1) for all trees\n",
    "    return trained_trees\n",
    "def my_forest_ada_predict_equal_weights(trained_trees, features):\n",
    "    votes = np.zeros(features.shape[0])\n",
    "    for tree, _ in trained_trees:\n",
    "        predictions = tree.predict(features)\n",
    "        votes += predictions\n",
    "    final_predictions = np.sign(votes)\n",
    "    return final_predictions\n",
    "def load_data(file1, file2):\n",
    "    def read_file(file_path, label):\n",
    "        features, targets = [], []\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                values = list(map(float, line.strip().split('\\t')[1:]))\n",
    "                features.append(values)\n",
    "                targets.append(label)\n",
    "        return np.array(features), np.array(targets)\n",
    "\n",
    "    features1, targets1 = read_file(file1, 1)\n",
    "    features2, targets2 = read_file(file2, -1)\n",
    "\n",
    "    features = np.vstack([features1, features2])\n",
    "    targets = np.concatenate([targets1, targets2])\n",
    "\n",
    "    shuffle_indices = np.random.permutation(len(targets))\n",
    "    return features[shuffle_indices], targets[shuffle_indices]\n",
    "X, Y = load_data('sun_exposed.txt', 'not_sun_exposed.txt')\n",
    "n_trees_range = range(1, 21)\n",
    "standard_accuracies = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_range):\n",
    "    ada_model = my_forest_ada_train(X, Y, n_trees)\n",
    "    predictions = my_forest_ada_predict(ada_model, X)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    standard_accuracies.append(accuracy)\n",
    "    \n",
    "    \n",
    "equal_weights_accuracies = []\n",
    "\n",
    "for n_trees in tqdm(n_trees_range):\n",
    "    equal_model = my_forest_ada_train_equal_weights(X, Y, n_trees)\n",
    "    predictions = my_forest_ada_predict_equal_weights(equal_model, X)\n",
    "    accuracy = np.mean(predictions == Y)\n",
    "    equal_weights_accuracies.append(accuracy)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_trees_range, standard_accuracies, marker='o', label='Standard AdaBoost')\n",
    "plt.plot(n_trees_range, equal_weights_accuracies, marker='s', label='Equal Weights AdaBoost')\n",
    "plt.title('Accuracy vs Number of Trees')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "aff707764a22f11f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:14<00:33,  2.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 72\u001B[0m\n\u001B[0;32m     69\u001B[0m standard_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n_trees \u001B[38;5;129;01min\u001B[39;00m tqdm(n_trees_range):\n\u001B[1;32m---> 72\u001B[0m     ada_model \u001B[38;5;241m=\u001B[39m my_forest_ada_train(X, Y, n_trees)\n\u001B[0;32m     73\u001B[0m     predictions \u001B[38;5;241m=\u001B[39m my_forest_ada_predict(ada_model, X)\n\u001B[0;32m     74\u001B[0m     accuracy \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(predictions \u001B[38;5;241m==\u001B[39m Y)\n",
      "Cell \u001B[1;32mIn[1], line 14\u001B[0m, in \u001B[0;36mmy_forest_ada_train\u001B[1;34m(features, labels, n_trees)\u001B[0m\n\u001B[0;32m     12\u001B[0m stump\u001B[38;5;241m.\u001B[39mfit(features, labels, sample_weight\u001B[38;5;241m=\u001B[39msample_weights)\n\u001B[0;32m     13\u001B[0m predictions \u001B[38;5;241m=\u001B[39m stump\u001B[38;5;241m.\u001B[39mpredict(features)\n\u001B[1;32m---> 14\u001B[0m error \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(sample_weights \u001B[38;5;241m*\u001B[39m (predictions \u001B[38;5;241m!=\u001B[39m labels)) \u001B[38;5;241m/\u001B[39m np\u001B[38;5;241m.\u001B[39msum(sample_weights)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m error \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.5\u001B[39m:\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mC:\\ProgramData\\miniconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2172\u001B[0m, in \u001B[0;36m_sum_dispatcher\u001B[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001B[0m\n\u001B[0;32m   2102\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2103\u001B[0m \u001B[38;5;124;03m    Clip (limit) the values in an array.\u001B[39;00m\n\u001B[0;32m   2104\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2167\u001B[0m \n\u001B[0;32m   2168\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   2169\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclip\u001B[39m\u001B[38;5;124m'\u001B[39m, a_min, a_max, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 2172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_sum_dispatcher\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   2173\u001B[0m                     initial\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, where\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   2174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (a, out)\n\u001B[0;32m   2177\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_sum_dispatcher)\n\u001B[0;32m   2178\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msum\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, keepdims\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue,\n\u001B[0;32m   2179\u001B[0m         initial\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue, where\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39m_NoValue):\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observations\n",
    "Standard AdaBoost:\n",
    "\n",
    "Accuracy improves as the number of trees increases, but may plateau or decrease slightly due to overfitting.\n",
    "Dynamic weighting allows the algorithm to focus on harder-to-classify samples, improving robustness.\n",
    "Equal Weights AdaBoost:\n",
    "\n",
    "Accuracy improves initially but plateaus much earlier than Standard AdaBoost.\n",
    "Without adaptive weights, the model cannot emphasize harder-to-classify samples, leading to reduced performance.\n",
    "Impact of Adaptive Weighting:\n",
    "\n",
    "Adaptive weighting enhances AdaBoost's ability to address difficult samples, improving overall accuracy.\n",
    "The equal-weight version essentially mimics a simple ensemble method without the boosting aspect.\n",
    "These differences demonstrate the importance of adaptive sample weighting in AdaBoost's performance."
   ],
   "id": "d1c01a0e62087f7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8eb06949d5b3ff5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
