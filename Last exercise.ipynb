{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-07T12:36:52.006162Z",
     "start_time": "2025-02-07T12:36:51.981708Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_threshold(x, lam):\n",
    "    \"\"\"Soft-thresholding operator S(x, lam) = sign(x)*max(|x|-lam, 0).\"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - lam, 0.0)\n",
    "\n",
    "def lasso_coordinate_descent(X, y, lam=1.0, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Solve Lasso: 1/2 * ||y - Xbeta||^2 + lam * ||beta||_1\n",
    "    using cyclical coordinate descent.\n",
    "    \"\"\"\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Precompute\n",
    "    V = X.T @ X  # p x p\n",
    "    s = X.T @ y  # p x 1\n",
    "\n",
    "    beta = np.zeros(p)  # initialize coefficients\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        beta_old = beta.copy()\n",
    "\n",
    "        for j in range(p):\n",
    "            # partial residual: s_j - sum_{k != j} V_{kj} * beta_k\n",
    "            r_j = s[j] - np.dot(V[j, :], beta) + V[j, j]*beta[j]\n",
    "\n",
    "            # Update beta_j\n",
    "            beta[j] = soft_threshold(r_j, lam) / (V[j, j] + 1e-12)  # add small epsilon to avoid /0\n",
    "\n",
    "        # check for convergence\n",
    "        if np.linalg.norm(beta - beta_old) < tol:\n",
    "            break\n",
    "\n",
    "    return beta\n",
    "\n",
    "\n",
    "# --- Validation on simulated data ---\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(123)\n",
    "\n",
    "    # Generate simulated data\n",
    "    n, p = 200, 10\n",
    "    X = np.random.randn(n, p)\n",
    "    # True beta with only a few non-zeros\n",
    "    beta_true = np.array([5, -3, 0, 0, 2, 0, 0, 0, 0, 0], dtype=float)\n",
    "    # Response with noise\n",
    "    y = X @ beta_true + 0.5 * np.random.randn(n)\n",
    "\n",
    "    lam = 1.0\n",
    "    beta_est = lasso_coordinate_descent(X, y, lam=lam, max_iter=1000, tol=1e-8)\n",
    "    print(\"True beta:\", beta_true)\n",
    "    print(\"Estimated beta:\", beta_est)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True beta: [ 5. -3.  0.  0.  2.  0.  0.  0.  0.  0.]\n",
      "Estimated beta: [ 5.01651435e+00 -2.95409981e+00  0.00000000e+00 -2.35553764e-02\n",
      "  1.99454076e+00  4.49970095e-04  4.32995799e-03 -2.45489737e-02\n",
      " -1.20736754e-02 -1.35658189e-02]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T12:36:52.126913Z",
     "start_time": "2025-02-07T12:36:52.093567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def soft_threshold_vector(z, lam):\n",
    "    \"\"\"Apply soft-thresholding to a vector z with threshold lam.\"\"\"\n",
    "    return np.sign(z) * np.maximum(np.abs(z) - lam, 0)\n",
    "\n",
    "def graphical_lasso(S, lam=0.1, max_iter=100, tol=1e-4):\n",
    "    \"\"\"\n",
    "    Graphical Lasso using the block-coordinate descent (Friedman et al. style).\n",
    "\n",
    "    S: empirical covariance matrix (p x p)\n",
    "    lam: regularization parameter\n",
    "    Returns: (Theta, Sigma) the estimated precision and covariance\n",
    "    \"\"\"\n",
    "    p = S.shape[0]\n",
    "    # Initialize Theta (precision) -- e.g. with the diagonal of S or identity\n",
    "    # A typical choice: invert (S + lam * I) to have a strictly positive definite\n",
    "    W = np.linalg.inv(S + lam * np.eye(p))\n",
    "    Theta = W.copy()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        Theta_old = Theta.copy()\n",
    "\n",
    "        for j in range(p):\n",
    "            # Partition indices\n",
    "            not_j = np.arange(p) != j\n",
    "\n",
    "            S11 = S[not_j][:, not_j]\n",
    "            s12 = S[not_j, j]\n",
    "\n",
    "            # Theta11 => block (not_j, not_j)\n",
    "            # We extract the sub-block we will modify\n",
    "            T11 = Theta[not_j][:, not_j]\n",
    "            t12 = Theta[not_j, j]\n",
    "\n",
    "            # Solve for new t12 using Lasso-like subproblem:\n",
    "            #   argmin_b  b^T T11 b - 2 s12^T b + lam * ||b||_1\n",
    "\n",
    "\n",
    "            # We define: W_11 = T11^{-1} (the sub‐cov)\n",
    "            W_11 = np.linalg.inv(T11)\n",
    "            gamma = - W_11 @ s12  # “pseudo” target (see derivations in Friedman (2008))\n",
    "\n",
    "            # Now solve the Lasso problem for b:\n",
    "            #   argmin_b  1/2 * b^T W_11^-1 b + ...\n",
    "            # but let's do a simpler approach: coordinate descent in practice.\n",
    "\n",
    "            # Initialize b with current t12\n",
    "            b = t12.copy()\n",
    "\n",
    "            # Simple coordinate descent for the subproblem\n",
    "            for it_inner in range(50):\n",
    "                b_old = b.copy()\n",
    "                for i_idx in range(len(b)):\n",
    "                    r_i = gamma[i_idx] - (W_11[i_idx, :] @ b) + W_11[i_idx, i_idx] * b[i_idx]\n",
    "                    # soft threshold\n",
    "                    b[i_idx] = soft_threshold(r_i, lam) / W_11[i_idx, i_idx]\n",
    "\n",
    "                if np.linalg.norm(b - b_old) < 1e-8:\n",
    "                    break\n",
    "\n",
    "            # Update t12 in Theta\n",
    "            t12_new = b\n",
    "\n",
    "            # Next, we need to update Theta_{jj} accordingly\n",
    "            #formula: Theta_{jj} = 1 / ( w_{jj} - w_{j,-j} * b )\n",
    "            # Then we set Theta_jj to the scalar:\n",
    "\n",
    "            # Compute the value for Theta_{j,j}\n",
    "            denom = S[j, j] - (s12.T @ t12_new)\n",
    "            Theta_jj_new = 1.0 / max(denom, 1e-12)\n",
    "\n",
    "            # Put the updates back\n",
    "            Theta[not_j, j] = t12_new\n",
    "            Theta[j, not_j] = t12_new\n",
    "            Theta[j, j] = Theta_jj_new\n",
    "\n",
    "        # check for convergence\n",
    "        if np.linalg.norm(Theta - Theta_old, ord='fro') < tol:\n",
    "            break\n",
    "\n",
    "    # Symmetrize just in case of numerical issues\n",
    "    Theta = 0.5 * (Theta + Theta.T)\n",
    "\n",
    "    # Invert Theta to get the covariance\n",
    "    Sigma_est = np.linalg.inv(Theta)\n",
    "\n",
    "    return Theta, Sigma_est\n",
    "\n",
    "\n",
    "# --- Validation using simulated data ---\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(42)\n",
    "    p = 5\n",
    "    n = 500\n",
    "\n",
    "    # Generate a random precision matrix Theta_true with some sparse off-diagonal\n",
    "    # e.g., a random diagonal + a few random off-diagonals\n",
    "    A = 0.6 * np.random.randn(p, p)\n",
    "    # Make it symmetric\n",
    "    A = 0.5*(A + A.T)\n",
    "    # Shift the diagonal to ensure positive definite\n",
    "    diag_add = p * np.eye(p)\n",
    "    Theta_true = A + diag_add\n",
    "    Theta_true = 0.5 * (Theta_true + Theta_true.T)  # symmetrize\n",
    "\n",
    "    # Check positive definiteness (do a shift if needed)\n",
    "    # We'll assume it's PD enough. Or use e.g. a nearPD approach if needed.\n",
    "\n",
    "    # Generate data X ~ N(0, Sigma_true) where Sigma_true = Theta_true^{-1}\n",
    "    Sigma_true = np.linalg.inv(Theta_true)\n",
    "    X = np.random.multivariate_normal(mean=np.zeros(p), cov=Sigma_true, size=n)\n",
    "\n",
    "    # Empirical covariance\n",
    "    S = np.cov(X, rowvar=False)\n",
    "\n",
    "    # Run Graphical Lasso\n",
    "    lam = 0.1\n",
    "    Theta_est, Sigma_est = graphical_lasso(S, lam=lam, max_iter=100, tol=1e-4)\n",
    "\n",
    "    print(\"True Precision:\\n\", Theta_true)\n",
    "    print(\"Estimated Precision:\\n\", Theta_est)\n",
    "\n",
    "    # Check special cases:\n",
    "    # 1) lambda -> 0: we should get close to the MLE, i.e. S^{-1}.\n",
    "    # 2) lambda -> large: we expect more shrinkage, eventually diagonal.\n"
   ],
   "id": "4bd1ae0b29fd02e9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Precision:\n",
      " [[ 5.29802849 -0.11172038  0.05528125  0.2882227   0.36944862]\n",
      " [-0.11172038  5.94752769  0.09051149 -0.44469165  0.09503512]\n",
      " [ 0.05528125  0.09051149  5.14517736 -0.47970987 -0.49721689]\n",
      " [ 0.2882227  -0.44469165 -0.47970987  4.45518555 -0.85111557]\n",
      " [ 0.36944862  0.09503512 -0.49721689 -0.85111557  4.67337037]]\n",
      "Estimated Precision:\n",
      " [[ 5.66514038  0.          0.          0.          0.        ]\n",
      " [ 0.          5.64671335  0.         -0.          0.        ]\n",
      " [ 0.          0.          5.77929504 -0.         -0.        ]\n",
      " [ 0.         -0.         -0.          4.14891882 -0.        ]\n",
      " [ 0.          0.         -0.         -0.          4.43464563]]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T12:36:52.231440Z",
     "start_time": "2025-02-07T12:36:52.229388Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7c2543f3345515cd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
